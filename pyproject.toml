[project]
name = "transformerlm"
version = "1.0.0"
description = "Diffusion LM mini stack: from-scratch core with tokenizer, streaming data, CLI, and lightweight benchmarks."
readme = "README.md"
requires-python = ">=3.11,<3.13"
dependencies = [
    "numpy",
    "pytest>=8.3.4",
    "jaxtyping>=0.3.0",
    "torch",
    "tqdm>=4.67.1",
    "wandb>=0.19.7",
    "einops>=0.8.1",
    "regex>=2024.11.6",
    "datasets>=3.0.1",
    "pillow>=10.0.0",
    "pydantic>=2.10",
    "safetensors>=0.4.5",
    "boto3>=1.34.0",
    "transformers>=4.57.3",
]

[tool.uv]
python-preference = "managed"

[tool.ruff]
line-length = 120

[tool.ruff.lint]
extend-select = ["UP"]
ignore = [
    "F722"
]

[project.scripts]
transformerlm-train = "cli.train:main"
transformerlm-infer = "cli.infer:main"
transformerlm-infer-image = "cli.infer_image:main"
transformerlm-sweep-infer = "cli.sweep_infer:main"
transformerlm-sweep-train = "cli.sweep_train:main"
transformerlm-train-tokenizer = "cli.train_tokenizer:main"
transformerlm-bench-infer = "benchmarking.bench_infer_latency:main"
transformerlm-bench-tokenizer = "benchmarking.bench_tokenizer:main"

[build-system]
requires = ["maturin>=1.7,<2"]
build-backend = "maturin"

[tool.maturin]
bindings = "pyo3"
module-name = "transformerlm.tokenizer_rust"
python-source = "."
manifest-path = "transformerlm/tokenizer_rust/Cargo.toml"

[tool.pytest.ini_options]
addopts = ["-q"]
markers = [
  "slow: marks tests as slow (deselect with '-m \"not slow\"')",
  "gpu: requires CUDA/GPU",
  "cpu: requires only CPU resources",
]
