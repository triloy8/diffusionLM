[tokenizer]
merges_path = "./data/gpt2_merges.txt"
vocab_path = "./data/gpt2_vocab.json"
special_tokens_path = "./data/special_tokens.json"

[input]
text_list = [
  "The quick brown fox jumps over the lazy dog.",
  "To be, or not to be, that is the question.",
  "Hello, world!",
]

[benchmark]
repeats = 5
