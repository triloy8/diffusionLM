program: cli/sweep_train.py
project: diffusionLM-cli
command:
  - ${env}
  - uv
  - run
  - diffusionlm-sweep-train
  - --config
  - config/resources/train.toml
method: bayes
metric:
  goal: minimize
  name: metrics.train_loss_ema
parameters:
  logging.architecture:
    value: DiffusionLM
  logging.dataset:
    value: TinyStories
  training.batch_size:
    value: 68
  training.max_train_iteration:
    value: 1600
  training.max_val_iteration:
    value: 10
  training.val_freq_iteration:
    value: 250
  training.grad_accum_steps:
    value: 2
  checkpointing.ckpting_save_iter:
    value: 1000
  model.context_length:
    value: 512
  model.d_ff:
    value: 3072
  model.d_model:
    value: 768
  data.dataset_name:
    value: roneneldan/TinyStories
  data.dataset_config:
    value: ""
  data.train_split:
    value: train
  data.val_split:
    value: validation
  data.text_field:
    value: text
  data.pipeline_mode:
    value: rows
  data.pad_token_id:
    value: 50256
  data.pad_random_shift:
    value: true
  data.shuffle_buffer_size:
    value: 30000
  data.shuffle_seed:
    value: 3407
  model.device:
    value: cuda
  model.dtype:
    value: bfloat16
  model.vocab_size:
    value: 50259
  model.mask_token_id:
    value: 50258
  model.eot_token_id:
    value: 50257
  model.noise_epsilon:
    value: 0.001
  model.random_trunc_prob:
    value: 0
  optimizer.optimizer_name:
    value: muon
  optimizer.betas:
    value: [0.9, 0.95]
  optimizer.eps:
    value: 1e-10
  optimizer.grad_clip_max_l2_norm:
    value: 1.0
  model.num_heads:
    value: 16
  model.num_layers:
    value: 8
  model.rope_theta:
    value: 10000.0
  optimizer.cosine_cycle_iters:
    value: 50000
  optimizer.initial_learning_rate:
    value: 0.0001
  optimizer.max_learning_rate:
    value: 0.0005
  optimizer.warmup_iters:
    value: 500
  optimizer.weight_decay:
    value: 0.05
  optimizer.min_learning_rate:
    value: 0.00005
  optimizer.muon.hidden.max_learning_rate:
    distribution: log_uniform_values
    min: 0.007
    max: 0.014
  optimizer.muon.embed.max_learning_rate:
    distribution: log_uniform_values
    min: 0.012
    max: 0.025
  # optimizer.muon.head.max_learning_rate:
  #   distribution: log_uniform_values
  #   min: 0.0015
  #   max: 0.0045
  # optimizer.muon.scalar.max_learning_rate:
  #   distribution: log_uniform_values
  #   min: 0.0004
  #   max: 0.0012
