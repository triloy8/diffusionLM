[model]
vocab_size = 50258
context_length = 512
d_model = 768
num_layers = 8
num_heads = 16
d_ff = 3072
rope_theta = 10000.0
device = "cuda"
dtype = "bfloat16"
mask_token_id = 50257
eot_token_id = 50256
noise_epsilon = 0.001
random_trunc_prob = 0 # 0.01

[optimizer]
optimizer_name = "muon"
betas = [0.9, 0.95] # adamw
eps = 1e-10 # adamw
weight_decay = 0.05 # adamw
initial_learning_rate = 1e-4 # adamw, unused in cosine scheduler
max_learning_rate = 0.0015 # adamw
min_learning_rate = 0.00015 # adamw
warmup_iters = 0
cosine_cycle_iters = 50000
grad_clip_max_l2_norm = 1.0

[optimizer.muon.hidden]
initial_learning_rate = 0.01 # unused in cosine scheduler
max_learning_rate = 0.01
min_learning_rate = 0.001
momentum = 0.95
weight_decay = 0.0

[optimizer.muon.head]
initial_learning_rate = 0.003 # unused in cosine scheduler
max_learning_rate = 0.003
min_learning_rate = 0.0003
betas = [0.9, 0.95]
eps = 1e-10
weight_decay = 0.02

[optimizer.muon.embed]
initial_learning_rate = 0.01 # unused in cosine scheduler
max_learning_rate = 0.02
min_learning_rate = 0.002
betas = [0.9, 0.95]
eps = 1e-10
weight_decay = 0.005

[optimizer.muon.scalar]
initial_learning_rate = 0.0008 # unused in cosine scheduler
max_learning_rate = 0.0008
min_learning_rate = 0.00004
betas = [0.9, 0.95]
eps = 1e-10
weight_decay = 0.02

[training]
batch_size = 72
max_train_iteration = 50000
max_val_iteration = 10
val_freq_iteration = 125
seed = 3407
skip_validation = false
grad_accum_steps = 1

[checkpointing]
enabled = true
ckpting_save_iter = 1000
best_metric_name = "val_loss"
best_mode = "min"

[data]
runs_path = "./runs"
dataset_name = "roneneldan/TinyStories"
dataset_config = ""
train_split = "train"
val_split = "validation"
text_field = "text"
shuffle_buffer_size = 30000
shuffle_seed = 3407

[data.tokenizer]
vocab_path = "./data/vocab.json"
merges_path = "./data/merges.txt"
special_tokens_path = "./data/special_tokens.json"

[logging]
backend = "wandb"
architecture = "DiffusionLM"
dataset = "TinyStories"
log_activation_norms = true
log_weight_norms = true
val_log_every = 8
val_log_samples = 16

[wandb]
entity = "yiltro8-org"
project = "diffusion_lm"

[ddp]
backend = "nccl"               # gloo/nccl
num_nodes = 1                  # total nodes participating
num_gpus_per_node = 1          # processes launched per node (mp.spawn uses this)
node_rank = 0                  # index of this node
master_addr = "127.0.0.1"      # rank 0 hostname/IP
master_port = "29500"
bucket_size_mb = 200           # 0 = single bucket
nccl_p2p_disable = true
