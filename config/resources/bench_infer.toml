[tokenizer]
merges_path = "./data/gpt2_merges.txt"
vocab_path = "./data/gpt2_vocab.json"
special_tokens = ["<|endoftext|>", "<|mask|>"]

[model]
vocab_size = 50304
context_length = 256
d_model = 512
num_layers = 4
num_heads = 16
d_ff = 1344
rope_theta = 10000.0
device = "cuda"
dtype = "float16"
mask_token_id = 50257
noise_epsilon = 0.001
random_trunc_prob = 0.01

[checkpoint]
ckpt_path = "./runs/2025-10-14_10-42-53/5000.ckpt"

[inference]
prompt = "Once upon a time,"
steps = 256
total_length = 256
block_length = 128
temperature = 0.01
mask_id = 50257

[data]
np_dat_valid_path = "./data/TinyStoriesV2-GPT4-valid.dat"
total_val_tokens = 5535291

[benchmark]
warmup = 5
repeats = 10
steps = 64
synchronize = true
backward = true
optimizer_step = true
perplexity_max_batches = 16
perplexity_batch_size = 32
perplexity_seed = 0

# Optional optimizer section for training-step benchmarking
[optimizer]
lr = 0.0
betas = [0.9, 0.999]
eps = 1e-8
weight_decay = 0.0
grad_clip_max_l2_norm = 0.0
