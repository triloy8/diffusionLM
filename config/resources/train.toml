[model]
vocab_size = 50258
context_length = 256
d_model = 512
num_layers = 4
num_heads = 16
d_ff = 2048
rope_theta = 10000.0
device = "cuda"
dtype = "float32"
mask_token_id = 50257
eot_token_id = 50256
noise_epsilon = 0.001
random_trunc_prob = 0.01 # 0.01

[optimizer]
optimizer_name = "adamw"
betas = [0.9, 0.95] # adamw
eps = 1e-8 # adamw
weight_decay = 0.001 # adamw
initial_learning_rate = 1e-4 # adamw, unused in cosine scheduler
max_learning_rate = 0.001 # adamw
min_learning_rate = 0.0001 # adamw
warmup_iters = 500
cosine_cycle_iters = 40000
grad_clip_max_l2_norm = 1.0

[training]
batch_size = 80
max_train_iteration = 1600
max_val_iteration = 10
val_freq_iteration = 250
seed = 3407
skip_validation = false
grad_accum_steps = 2
train_loss_ema_decay = 0.99

[data]
runs_path = "./runs"
dataset_name = "roneneldan/TinyStories"
dataset_config = ""
train_split = "train"
val_split = "validation"
text_field = "text"
pipeline_mode = "packed" # rows/packed
pad_token_id = 50256
pad_random_shift = true
shuffle_buffer_size = 30000
shuffle_seed = 3407

[data.tokenizer]
vocab_path = "./data/vocab.json"
merges_path = "./data/merges.txt"
special_tokens_path = "./data/special_tokens.json"

[logging]
backend = "wandb"
architecture = "DiffusionLM"
dataset = "TinyStories"
log_activation_norms = true
log_weight_norms = true
log_grad_norms = true
log_p_mask_bucket_loss = false
p_mask_bucket_edges = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
val_log_every = 8
val_log_samples = 1

[train_infer]
infer_every = 1000
prompts = ["Once upon a time,", "One day,"]
steps = 256
total_length = 256
block_length = 64
temperature = 0.6
top_p = 0.95
cfg_scale = 0.0
remasking = "random"
generation_mode = "diffusion"
logits_eos_inf = false
confidence_eos_eot_inf = false
seed = 3410

[wandb]
entity = "yiltro8-org"
project = "diffusion_lm"

[ddp]
backend = "nccl"               # gloo/nccl
num_nodes = 1                  # total nodes participating
num_gpus_per_node = 2          # processes launched per node (mp.spawn uses this)
node_rank = 0                  # index of this node
master_addr = "127.0.0.1"      # rank 0 hostname/IP
master_port = "29500"
bucket_size_mb = 200           # 0 = single bucket
nccl_p2p_disable = true

[checkpointing]
enabled = true
ckpting_save_iter = 1000
resume_optimizer = true
best_metric_name = "val_loss"
best_mode = "min"
# resume_from = "latest" # or "best" or path to manifest.json
# run_id = "2025-12-29_16-00-44"

# Remote storage is configured via env vars (see env/checkpointing.env.example).
