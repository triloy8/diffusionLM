[model]
vocab_size = 50258
context_length = 512
d_model = 768
num_layers = 8
num_heads = 16
d_ff = 3072
rope_theta = 10000.0
device = "cuda"
dtype = "bfloat16"
mask_token_id = 50257
eot_token_id = 50256
noise_epsilon = 0.001
random_trunc_prob = 0 # 0.01

[optimizer]
optimizer_name = "adamw"
betas = [0.9, 0.95] # adamw
eps = 1e-10 # adamw
weight_decay = 0.05 # adamw
initial_learning_rate = 1e-4 # adamw, unused in cosine scheduler
max_learning_rate = 5e-4 # adamw
min_learning_rate = 5e-5 # adamw
warmup_iters = 500
cosine_cycle_iters = 50000
grad_clip_max_l2_norm = 1.0

[training]
batch_size = 70
max_train_iteration = 50000
max_val_iteration = 10
val_freq_iteration = 125
seed = 3407
skip_validation = false
grad_accum_steps = 2

[data]
runs_path = "./runs"
dataset_name = "roneneldan/TinyStories"
dataset_config = ""
train_split = "train"
val_split = "validation"
text_field = "text"
pipeline_mode = "packed"
# pad_token_id = 0
shuffle_buffer_size = 30000
shuffle_seed = 3407

[data.tokenizer]
vocab_path = "./data/vocab.json"
merges_path = "./data/merges.txt"
special_tokens_path = "./data/special_tokens.json"

[logging]
backend = "wandb"
architecture = "DiffusionLM"
dataset = "TinyStories"
log_activation_norms = true
log_weight_norms = true
val_log_every = 8
val_log_samples = 16

[wandb]
entity = "yiltro8-org"
project = "diffusion_lm"

[ddp]
backend = "nccl"               # gloo/nccl
num_nodes = 1                  # total nodes participating
num_gpus_per_node = 2          # processes launched per node (mp.spawn uses this)
node_rank = 0                  # index of this node
master_addr = "127.0.0.1"      # rank 0 hostname/IP
master_port = "29500"
bucket_size_mb = 200           # 0 = single bucket
nccl_p2p_disable = true

[checkpointing]
enabled = true
ckpting_save_iter = 1000
resume_optimizer = true
best_metric_name = "val_loss"
best_mode = "min"
# resume_from = "latest" # or "best" or path to manifest.json
# run_id = "2025-12-29_16-00-44"

# Remote storage is configured via env vars (see env/checkpointing.env.example).
