[model]
vocab_size = 4097
context_length = 2048
d_model = 512
num_layers = 12
num_heads = 8
d_ff = 1344
rope_theta = 10000.0
attention_backend = "torch_sdpa"
attention_sdp_backend = "auto"
device = "cuda"
dtype = "float32"
mask_token_id = 4096
eot_token_id = 4095
noise_epsilon = 0.001
random_trunc_prob = 0.0 # 0.01

[optimizer]
optimizer_name = "adamw"
betas = [0.9, 0.95] # adamw
eps = 1e-8 # adamw
weight_decay = 0.1 # adamw
initial_learning_rate = 0.0001 # adamw, unused in cosine scheduler
max_learning_rate = 0.002 # adamw, used by const schedule
min_learning_rate = 0.0002 # adamw
warmup_iters = 200
cosine_cycle_iters = 60000
grad_clip_max_l2_norm = 3.0
lr_schedule = "cosine" # constant/cosine/constant_with_warmup

[training]
batch_size = 128
max_train_iteration = 120000
max_val_iteration = 10
val_freq_iteration = 100
seed = 3407
skip_validation = false
grad_accum_steps = 1
train_loss_ema_decay = 0.99
amp_enabled = true
amp_dtype = "bfloat16"
objective = "megadlm-diffusion" # diffusion/megadlm-diffusion/ar
eot_mask_loss = false
# repeat_masking_seed = 3407
# p_mask_override = 0.3
# deterministic_mask = false

[compile]
enabled = false
backend = "inductor"
mode = "default"
fullgraph = false
dynamic = false
# options = { triton.cudagraphs = false }

[data]
runs_path = "./runs"
dataset_name = "SimpleStories/SimpleStories"
dataset_config = ""
train_split = "train"
val_split = "test"
text_field = "story"
pipeline_mode = "megatron" # rows/packed/megatron
pad_token_id = 50256
pad_random_shift = false
shuffle_buffer_size = 0
cache_all = true
shuffle_seed = 3407
megatron_train_prefix = "./data/simplestories_train_text_document"
megatron_val_prefix = "./data/simplestories_test_text_document"

[data.tokenizer]
vocab_path = "./data/vocab_simplestories_4k.json"
merges_path = "./data/merges_simplestories_4k.txt"
special_tokens_path = "./data/special_tokens_simplestories_4k.json"

[logging]
backend = "wandb"
architecture = "DiffusionLM"
dataset = "SimpleStories"
log_activation_norms = true
log_weight_norms = true
log_grad_norms = true
log_p_mask_bucket_loss = false
p_mask_bucket_edges = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
val_log_every = 8
val_log_samples = 1

[train_infer]
infer_every = 1000
prompts = ["Silently,", "Suddenly,"]
steps = 512
total_length = 512
block_length = 256
temperature = 0.2
top_p = 0.95
cfg_scale = 0.0
remasking = "low_confidence"
generation_mode = "diffusion"
logits_eos_inf = false
confidence_eos_eot_inf = false
seed = 3410

[wandb]
entity = "yiltro8-org"
project = "diffusion_adamw_lr_simplestories_5"

[ddp]
backend = "nccl"               # gloo/nccl
num_nodes = 1                  # total nodes participating
num_gpus_per_node = 1          # processes launched per node (mp.spawn uses this)
node_rank = 0                  # index of this node
master_addr = "127.0.0.1"      # rank 0 hostname/IP
master_port = "29500"
bucket_size_mb = 200           # 0 = single bucket
nccl_p2p_disable = true

[checkpointing]
enabled = true
ckpting_save_iter = 1000
resume_optimizer = true
best_metric_name = "val_loss"
best_mode = "min"
# resume_from = "latest" # or "best" or path to manifest.json
# run_id = "2025-12-29_16-00-44"

# Remote storage is configured via env vars (see env/checkpointing.env.example).
