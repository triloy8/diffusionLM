[model]
vocab_size = 50304
context_length = 256
d_model = 512
num_layers = 4
num_heads = 16
d_ff = 1344
rope_theta = 10000.0
device = "cuda"
dtype = "float32"
mask_token_id = 50257
noise_epsilon = 0.001
random_trunc_prob = 0.01

[optimizer]
optimizer_name = "muon"
betas = [0.9, 0.95]
eps = 1e-8
weight_decay = 0.001
initial_learning_rate = 0.001
max_learning_rate = 0.001
min_learning_rate = 0.0001
warmup_iters = 50
cosine_cycle_iters = 4500
grad_clip_max_l2_norm = 1.0

[optimizer.muon.hidden]
initial_learning_rate = 0.05
max_learning_rate = 0.05
min_learning_rate = 0.05
momentum = 0.95
weight_decay = 0.0

[optimizer.muon.head]
initial_learning_rate = 0.22
max_learning_rate = 0.22
min_learning_rate = 0.22
betas = [0.8, 0.95]
eps = 1e-10
weight_decay = 0.0

[optimizer.muon.embed]
initial_learning_rate = 0.6
max_learning_rate = 0.6
min_learning_rate = 0.6
betas = [0.8, 0.95]
eps = 1e-10
weight_decay = 0.0

[optimizer.muon.scalar]
initial_learning_rate = 0.04
max_learning_rate = 0.04
min_learning_rate = 0.04
betas = [0.8, 0.95]
eps = 1e-10
weight_decay = 0.0

[training]
batch_size = 8
max_train_iteration = 5000
max_val_iteration = 10
val_freq_iteration = 125
ckpting_save_iter = 1000
seed = 3407
skip_validation = false

[data]
runs_path = "./runs"
dataset_name = "trixyL/tiny-story"
dataset_config = "TinyStoriesV2-GPT4"
train_split = "train"
val_split = "validation"
text_field = "text"
shuffle_buffer_size = 10000
shuffle_seed = 3407

[data.tokenizer]
vocab_path = "./data/gpt2_vocab.json"
merges_path = "./data/gpt2_merges.txt"
special_tokens_path = "./data/special_tokens.json"

[logging]
backend = "wandb"
architecture = "TransformerLM"
dataset = "TinyStoriesV2-GPT4"

[wandb]
entity = "yiltro8-org"
project = "diffusion_lm"
